{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk     \n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "DATA_DIR = os.path.join( '..', 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>lenght</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>SMS_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "      <td>111</td>\n",
       "      <td>['gone', 'until', 'jurong', 'point', ',', 'cra...</td>\n",
       "      <td>['go', 'until', 'jurong', 'point', ',', 'crazy...</td>\n",
       "      <td>['go', 'jurong', 'point', ',', 'crazy', '..', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>['ok', 'lar', '...', 'joke', 'wif', 'you', 'on...</td>\n",
       "      <td>['ok', 'lar', '...', 'joke', 'wif', 'oni', '...']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                                SMS  lenght  \\\n",
       "0      0  gone until jurong point, crazy.. available onl...     111   \n",
       "1      0                    ok lar... joking wif you oni...      29   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  ['gone', 'until', 'jurong', 'point', ',', 'cra...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ['go', 'until', 'jurong', 'point', ',', 'crazy...   \n",
       "1  ['ok', 'lar', '...', 'joke', 'wif', 'you', 'on...   \n",
       "\n",
       "                               SMS_without_stopwords  \n",
       "0  ['go', 'jurong', 'point', ',', 'crazy', '..', ...  \n",
       "1  ['ok', 'lar', '...', 'joke', 'wif', 'oni', '...']  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(PROCESSED_DATA_DIR,\"sms.csv\")\n",
    "sms=pd.read_csv(file_path)\n",
    "sms.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenar un modelo de regresión logística, necesitas convertir tus datos de texto en\n",
    "#características numéricas. Puedes hacer esto utilizando un enfoque de \"bag of words\"\n",
    "#(bolsa de palabras) con CountVectorizer:\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sms['SMS'])\n",
    "tamanio_X = X.size\n",
    "X_new=['get','go','let','come','want','ok','good','know','call','like','text','free','mobile','win','stop','claim','reply','message','prize','tone']\n",
    "list_key=list(range(20))\n",
    "new_columns = list_key[0:20]\n",
    "X_new=X[new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5263, 19)\n",
      "(5263, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>lenght</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>SMS_without_stopwords</th>\n",
       "      <th>X_3544</th>\n",
       "      <th>X_4641</th>\n",
       "      <th>X_2177</th>\n",
       "      <th>X_8366</th>\n",
       "      <th>...</th>\n",
       "      <th>X_7703</th>\n",
       "      <th>X_3390</th>\n",
       "      <th>X_5148</th>\n",
       "      <th>X_8533</th>\n",
       "      <th>X_7365</th>\n",
       "      <th>X_2081</th>\n",
       "      <th>X_6480</th>\n",
       "      <th>X_5049</th>\n",
       "      <th>X_6151</th>\n",
       "      <th>X_7899</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "      <td>111</td>\n",
       "      <td>['gone', 'until', 'jurong', 'point', ',', 'cra...</td>\n",
       "      <td>['go', 'until', 'jurong', 'point', ',', 'crazy...</td>\n",
       "      <td>['go', 'jurong', 'point', ',', 'crazy', '..', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>['ok', 'lar', '...', 'joke', 'wif', 'you', 'on...</td>\n",
       "      <td>['ok', 'lar', '...', 'joke', 'wif', 'oni', '...']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                                SMS  lenght  \\\n",
       "0      0  gone until jurong point, crazy.. available onl...     111   \n",
       "1      0                    ok lar... joking wif you oni...      29   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  ['gone', 'until', 'jurong', 'point', ',', 'cra...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ['go', 'until', 'jurong', 'point', ',', 'crazy...   \n",
       "1  ['ok', 'lar', '...', 'joke', 'wif', 'you', 'on...   \n",
       "\n",
       "                               SMS_without_stopwords  X_3544  X_4641  X_2177  \\\n",
       "0  ['go', 'jurong', 'point', ',', 'crazy', '..', ...       0       0       0   \n",
       "1  ['ok', 'lar', '...', 'joke', 'wif', 'oni', '...']       0       0       0   \n",
       "\n",
       "   X_8366  ...  X_7703  X_3390  X_5148  X_8533  X_7365  X_2081  X_6480  \\\n",
       "0       0  ...       0       0       0       0       0       0       0   \n",
       "1       0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "   X_5049  X_6151  X_7899  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supongamos que ya tienes 'sms1' y 'list_key' definidos\n",
    "# También, supongo que 'sms1' es un DataFrame de Pandas.\n",
    "\n",
    "# Crear el vectorizador y ajustar/transformar los datos\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sms['SMS'])\n",
    "\n",
    "# Obtener las columnas correspondientes a las palabras en 'list_key'\n",
    "#list_key = [\"want\",\"get\"]\n",
    "list_key = X_new=['get','go','let','come','want','ok','good','know','call','like','text','free','mobile','win','stop','claim','reply','message','prize','tone']\n",
    "selected_columns = [vectorizer.vocabulary_[word] for word in list_key if word in vectorizer.vocabulary_]\n",
    "\n",
    "# Seleccionar solo las columnas relevantes\n",
    "X_new = X[:, selected_columns]\n",
    "\n",
    "# Convertir X_new a un DataFrame de Pandas\n",
    "X_new_df = pd.DataFrame(X_new.toarray(), columns=['X_' + str(i) for i in selected_columns])\n",
    "\n",
    "# Concatenar X_new_df con el conjunto de datos original sms1\n",
    "sms = pd.concat([sms, X_new_df], axis=1)\n",
    "\n",
    "# Verificar las dimensiones de X_new\n",
    "print(X_new.shape)\n",
    "\n",
    "print(sms.shape)\n",
    "sms.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'SMS', 'lenght', 'Tokens', 'lemmatized_text',\n",
       "       'SMS_without_stopwords', 'X_3544', 'X_4641', 'X_2177', 'X_8366',\n",
       "       'X_5574', 'X_3619', 'X_4498', 'X_1841', 'X_4670', 'X_7703', 'X_3390',\n",
       "       'X_5148', 'X_8533', 'X_7365', 'X_2081', 'X_6480', 'X_5049', 'X_6151',\n",
       "       'X_7899'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check all the columns\n",
    "sms.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_final =sms[['Label', 'X_3544', 'X_4641', 'X_2177',\n",
    "       'X_8366', 'X_5574', 'X_3619', 'X_4498', 'X_1841', 'X_4670', 'X_7703',\n",
    "       'X_3390', 'X_5148', 'X_8533', 'X_7365', 'X_2081', 'X_6480', 'X_5049',\n",
    "       'X_6151', 'X_7899']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_final separated in train validation and test\n",
    "sms_train, sms_test = train_test_split(sms_final, test_size=0.2, random_state=42)\n",
    "\n",
    "sms_train, sms_val = train_test_split(sms_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in csv the train, validation and test in the folder processed\n",
    "sms_train.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_train.csv\"),index=False)\n",
    "sms_val.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_val.csv\"),index=False)\n",
    "sms_test.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_test.csv\"),index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
