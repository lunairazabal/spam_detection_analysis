{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk     \n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from wordcloud import WordCloud\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "DATA_DIR = os.path.join( '..', 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>lenght</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>SMS_without_stopwords</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "      <td>111</td>\n",
       "      <td>['gone', 'until', 'jurong', 'point', ',', 'cra...</td>\n",
       "      <td>['go', 'until', 'jurong', 'point', ',', 'crazy...</td>\n",
       "      <td>gone jurong point, crazy.. available bugis gre...</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>ok lar... joking wif oni...</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                                SMS  lenght  \\\n",
       "0      0  gone until jurong point, crazy.. available onl...     111   \n",
       "1      0                    ok lar... joking wif you oni...      29   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  ['gone', 'until', 'jurong', 'point', ',', 'cra...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ['go', 'until', 'jurong', 'point', ',', 'crazy...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                               SMS_without_stopwords  \\\n",
       "0  gone jurong point, crazy.. available bugis gre...   \n",
       "1                        ok lar... joking wif oni...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  gone until jurong point, crazy.. available onl...  \n",
       "1                    ok lar... joking wif you oni...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(PROCESSED_DATA_DIR,\"sms.csv\")\n",
    "sms=pd.read_csv(file_path)\n",
    "sms.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para entrenar un modelo de regresión logística, necesitas convertir tus datos de texto en\n",
    "#características numéricas. Puedes hacer esto utilizando un enfoque de \"bag of words\"\n",
    "#(bolsa de palabras) con CountVectorizer:\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sms['SMS'])\n",
    "tamanio_X = X.size\n",
    "X_new=['gone','want','get','one','please','message','send','call','come','know','free','mobile','claim','reply','good','stop','new','got','like','text']\n",
    "list_key=list(range(8))\n",
    "new_columns = list_key[0:8]\n",
    "X_new=X[new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5263, 20)\n",
      "(5263, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>lenght</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>SMS_without_stopwords</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>X_3615</th>\n",
       "      <th>X_8366</th>\n",
       "      <th>X_3544</th>\n",
       "      <th>...</th>\n",
       "      <th>X_3390</th>\n",
       "      <th>X_5148</th>\n",
       "      <th>X_2081</th>\n",
       "      <th>X_6480</th>\n",
       "      <th>X_3619</th>\n",
       "      <th>X_7365</th>\n",
       "      <th>X_5391</th>\n",
       "      <th>X_3637</th>\n",
       "      <th>X_4670</th>\n",
       "      <th>X_7703</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "      <td>111</td>\n",
       "      <td>['gone', 'until', 'jurong', 'point', ',', 'cra...</td>\n",
       "      <td>['go', 'until', 'jurong', 'point', ',', 'crazy...</td>\n",
       "      <td>gone jurong point, crazy.. available bugis gre...</td>\n",
       "      <td>gone until jurong point, crazy.. available onl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>['ok', 'lar', '...', 'joking', 'wif', 'you', '...</td>\n",
       "      <td>ok lar... joking wif oni...</td>\n",
       "      <td>ok lar... joking wif you oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                                SMS  lenght  \\\n",
       "0      0  gone until jurong point, crazy.. available onl...     111   \n",
       "1      0                    ok lar... joking wif you oni...      29   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  ['gone', 'until', 'jurong', 'point', ',', 'cra...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ['go', 'until', 'jurong', 'point', ',', 'crazy...   \n",
       "1  ['ok', 'lar', '...', 'joking', 'wif', 'you', '...   \n",
       "\n",
       "                               SMS_without_stopwords  \\\n",
       "0  gone jurong point, crazy.. available bugis gre...   \n",
       "1                        ok lar... joking wif oni...   \n",
       "\n",
       "                                      processed_text  X_3615  X_8366  X_3544  \\\n",
       "0  gone until jurong point, crazy.. available onl...       1       0       0   \n",
       "1                    ok lar... joking wif you oni...       0       0       0   \n",
       "\n",
       "   ...  X_3390  X_5148  X_2081  X_6480  X_3619  X_7365  X_5391  X_3637  \\\n",
       "0  ...       0       0       0       0       0       0       0       1   \n",
       "1  ...       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   X_4670  X_7703  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supongamos que ya tienes 'sms1' y 'list_key' definidos\n",
    "# También, supongo que 'sms1' es un DataFrame de Pandas.\n",
    "\n",
    "# Crear el vectorizador y ajustar/transformar los datos\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sms['SMS'])\n",
    "\n",
    "# Obtener las columnas correspondientes a las palabras en 'list_key'\n",
    "#list_key = [\"want\",\"get\"]\n",
    "list_key = X_new=['gone','want','get','one','please','message','send','call','come','know','free','mobile','claim','reply','good','stop','new','got','like','text']\n",
    "selected_columns = [vectorizer.vocabulary_[word] for word in list_key if word in vectorizer.vocabulary_]\n",
    "\n",
    "# Seleccionar solo las columnas relevantes\n",
    "X_new = X[:, selected_columns]\n",
    "\n",
    "# Convertir X_new a un DataFrame de Pandas\n",
    "X_new_df = pd.DataFrame(X_new.toarray(), columns=['X_' + str(i) for i in selected_columns])\n",
    "\n",
    "# Concatenar X_new_df con el conjunto de datos original sms1\n",
    "sms = pd.concat([sms, X_new_df], axis=1)\n",
    "\n",
    "# Verificar las dimensiones de X_new\n",
    "print(X_new.shape)\n",
    "\n",
    "# Verificar las dimensiones de sms1 después de la concatenación\n",
    "print(sms.shape)\n",
    "sms.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'SMS', 'lenght', 'Tokens', 'lemmatized_text',\n",
       "       'SMS_without_stopwords', 'processed_text', 'X_3615', 'X_8366', 'X_3544',\n",
       "       'X_5601', 'X_5957', 'X_5049', 'X_6814', 'X_1841', 'X_2177', 'X_4498',\n",
       "       'X_3390', 'X_5148', 'X_2081', 'X_6480', 'X_3619', 'X_7365', 'X_5391',\n",
       "       'X_3637', 'X_4670', 'X_7703'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check all the columns\n",
    "sms.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_final =sms[['Label','X_3615', 'X_8366', 'X_3544',\n",
    "       'X_5601', 'X_5957', 'X_5049', 'X_6814', 'X_1841', 'X_2177', 'X_4498',\n",
    "       'X_3615', 'X_8366', 'X_3544', 'X_5601', 'X_5957', 'X_5049', 'X_6814',\n",
    "       'X_1841', 'X_2177', 'X_4498', 'X_3390', 'X_5148', 'X_2081', 'X_6480',\n",
    "       'X_3619', 'X_7365', 'X_5391', 'X_3637', 'X_4670', 'X_7703']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_final separated in train validation and test\n",
    "sms_train, sms_test = train_test_split(sms_final, test_size=0.2, random_state=42)\n",
    "\n",
    "sms_train, sms_val = train_test_split(sms_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inicializar y entrenar un vectorizador TF-IDF\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# sms_train = vectorizer.fit_transform(sms_train['SMS']).todense()\n",
    "# sms_val=vectorizer.transform(sms_val['SMS']).todense()\n",
    "# sms_test = vectorizer.transform(sms_test['SMS']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save in csv the train, validation and test in the folder processed\n",
    "sms_train.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_train.csv\"),index=False)\n",
    "sms_val.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_val.csv\"),index=False)\n",
    "sms_test.to_csv(os.path.join(PROCESSED_DATA_DIR,\"sms_test.csv\"),index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
